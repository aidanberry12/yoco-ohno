{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PsUNSlZmQa6",
        "outputId": "035d33c6-9b5f-4337-9a5f-2024a39f0419"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks/Group Project\n",
            "args.py\t\tdata_loader.py\t   model.py\ttargets.pt\n",
            "build_vocab.py\tdata.pt\t\t   output\ttrain.py\n",
            "data\t\tingr2recipe.ipynb  __pycache__\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Change directory to the package folder\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks/Group Project/'\n",
        "\n",
        "# Verify the contents of the current folder\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQrJiCL5n-kK",
        "outputId": "a4d11a5f-a4a1-4450-ddec-7635726ed39a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "tcmalloc: large alloc 1405722624 bytes == 0x5579ebb80000 @  0x7fdccf3fb1e7 0x5579e8668338 0x5579e8632ad7 0x5579e87b1575 0x5579e874b8b8 0x5579e86363a2 0x5579e8714c6e 0x5579e874b832 0x5579e86363a2 0x5579e86395f6 0x5579e87b4c03 0x5579e8635ff9 0x5579e8727d4d 0x5579e86a9ec8 0x5579e86a4cdd 0x5579e863788a 0x5579e86a9d30 0x5579e86a4cdd 0x5579e863788a 0x5579e86a58f6 0x5579e86377aa 0x5579e86a58f6 0x5579e86a4a2e 0x5579e86a4723 0x5579e876e812 0x5579e876eb8d 0x5579e876ea36 0x5579e8746183 0x5579e8745e2c 0x7fdcce1e5c87 0x5579e8745d0a\n",
            "tcmalloc: large alloc 1405722624 bytes == 0x557a3f81a000 @  0x7fdccf3fb1e7 0x5579e8668338 0x5579e867d66c 0x5579e870ae13 0x5579e8635ff9 0x5579e8635ef0 0x5579e86aa123 0x5579e86a4a2e 0x5579e8637f21 0x5579e86373d0 0x5579e8639358 0x5579e8716441 0x5579e87b4c31 0x5579e8635ff9 0x5579e8727d4d 0x5579e86a9ec8 0x5579e86a4cdd 0x5579e863788a 0x5579e86a9d30 0x5579e86a4cdd 0x5579e863788a 0x5579e86a58f6 0x5579e86377aa 0x5579e86a58f6 0x5579e86a4a2e 0x5579e86a4723 0x5579e876e812 0x5579e876eb8d 0x5579e876ea36 0x5579e8746183 0x5579e8745e2c\n",
            "Loaded data.\n",
            "Found 1029720 recipes in the dataset.\n",
            "loading pre-extracted word counters\n",
            "Total ingr vocabulary size: 1488\n",
            "Total token vocabulary size: 23031\n",
            "1029720it [15:29, 1107.53it/s]\n",
            "Dataset size:\n",
            "train : 645114\n",
            "val : 138743\n",
            "test : 138070\n",
            "tcmalloc: large alloc 2147483648 bytes == 0x557adec72000 @  0x7fdccf3fb1e7 0x5579e8668da8 0x5579e86eaabf 0x5579e86e768e 0x5579e86e7bf1 0x5579e86e8652 0x5579e86e7b52 0x5579e86e6d9c 0x5579e85d86f2 0x5579e863611c 0x5579e8635ef0 0x5579e86aa123 0x5579e86377aa 0x5579e86a58f6 0x5579e86a4a2e 0x5579e86a4723 0x5579e876e812 0x5579e876eb8d 0x5579e876ea36 0x5579e8746183 0x5579e8745e2c 0x7fdcce1e5c87 0x5579e8745d0a\n",
            "tcmalloc: large alloc 1778032640 bytes == 0x5579eb2d4000 @  0x7fdccf3fc2a4 0x5579e863317c 0x5579e86ef0e2 0x5579e86e766f 0x5579e86e7bcc 0x5579e86e7b82 0x5579e86e8607 0x5579e86e7ba7 0x5579e86e6d9c 0x5579e85d86f2 0x5579e863611c 0x5579e8635ef0 0x5579e86aa123 0x5579e86377aa 0x5579e86a58f6 0x5579e86a4a2e 0x5579e86a4723 0x5579e876e812 0x5579e876eb8d 0x5579e876ea36 0x5579e8746183 0x5579e8745e2c 0x7fdcce1e5c87 0x5579e8745d0a\n"
          ]
        }
      ],
      "source": [
        "# This runs the build vocab file. Make sure you have a lot of ram.\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "!python build_vocab.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates the smaller train and test sets\n",
        "!python data_loader.py"
      ],
      "metadata": {
        "id": "NOzl20NYFjyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab65d6d-3fd3-4ab6-ee98-41f7729683b0"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcmalloc: large alloc 1073741824 bytes == 0x55acca1cc000 @  0x7f57ea7bb2a4 0x55ac83e8c712 0x55ac83d77840 0x55ac83dd511c 0x55ac83dd4ef0 0x55ac83e49123 0x55ac83dd67aa 0x55ac83e448f6 0x55ac83e43a2e 0x55ac83e43723 0x55ac83f0d812 0x55ac83f0db8d 0x55ac83f0da36 0x55ac83ee5183 0x55ac83ee4e2c 0x7f57e95a4c87 0x55ac83ee4d0a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-x9sLqD04WY",
        "outputId": "90ce2e9f-ff54-407a-8164-3bb99776a9a7"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading data for train\n",
            "Loading data for val\n",
            "Dataloader created\n",
            "-----------------------------------------------------------------------------------------\n",
            "Creating Model\n",
            "Defining Model Params\n",
            "Defining Pos Encodings\n",
            "Defining Ingredient Embeddings\n",
            "Defining Instruction Embeddings\n",
            "Creating Transformer Module\n",
            "Creating Final Linear Layer\n",
            "Initializing Weights\n",
            "Done Creating Model\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "Beginning Training\n",
            "0.637058012\n",
            "starting epoch 0\n",
            "training: train\n",
            "training: val\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   0 | valid loss  9.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "starting epoch 1\n",
            "training: train\n",
            "training: val\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | valid loss  8.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "starting epoch 2\n",
            "training: train\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 121, in <module>\n",
            "    total_loss=train(model,split,criterion,arg.batch_size,loader,instrs_vocab_size,num_batches)\n",
            "  File \"train.py\", line 35, in train\n",
            "    out = model(data, targets)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/Group Project/model.py\", line 47, in forward\n",
            "    output = self.transformer(src,tgt,tgt_mask=generate_square_subsequent_mask(tgt.size()[0],self.device))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\", line 141, in forward\n",
            "    memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\", line 198, in forward\n",
            "    output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\", line 340, in forward\n",
            "    x = self.norm2(x + self._ff_block(x))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\", line 355, in _ff_block\n",
            "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 1299, in relu\n",
            "    result = torch.relu(input)\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.90 GiB total capacity; 14.83 GiB already allocated; 27.75 MiB free; 14.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ingr2recipe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}